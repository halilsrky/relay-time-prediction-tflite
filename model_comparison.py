"""
4 FarklÄ± Model KarÅŸÄ±laÅŸtÄ±rmasÄ±: Linear Regression, XGBoost, LightGBM, Neural Network
TIME kolonunu tahmin etmek iÃ§in model performanslarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±r.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
import joblib
import os
warnings.filterwarnings('ignore')

# Sklearn modelleri ve metrikler
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import cross_val_score, validation_curve

# XGBoost ve LightGBM
import xgboost as xgb
import lightgbm as lgb

# Neural Network (MLP)
from sklearn.neural_network import MLPRegressor

# Plotting iÃ§in
plt.style.use('default')
sns.set_palette("husl")

class ModelComparison:
    """Model karÅŸÄ±laÅŸtÄ±rmasÄ± iÃ§in ana sÄ±nÄ±f"""
    
    def __init__(self, train_file='train.xlsx', test_file='test.xlsx'):
        """
        Initialize ile train ve test verilerini yÃ¼kle
        """
        print("=" * 60)
        print("MODEL KARÅILAÅTIRMA SISTEMI")
        print("=" * 60)
        
        # Veri yÃ¼kleme
        print("ğŸ“Š Veriler yÃ¼kleniyor...")
        self.train_df = pd.read_excel(train_file)
        self.test_df = pd.read_excel(test_file)
        
        print(f"âœ… Train verisi: {self.train_df.shape}")
        print(f"âœ… Test verisi: {self.test_df.shape}")
        
        # Veri Ã¶n iÅŸleme
        self.prepare_data()
        
        # Model sonuÃ§larÄ±nÄ± saklamak iÃ§in
        self.results = {}
        self.predictions = {}
        self.models = {}  # EÄŸitilmiÅŸ modelleri saklamak iÃ§in
        self.validation_results = {}  # Cross-validation sonuÃ§larÄ±
        
        # Ã‡Ä±ktÄ± klasÃ¶rleri oluÅŸtur
        self.output_dir = 'model_outputs'
        self.models_dir = 'saved_models'
        
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            print(f"ğŸ“ Ã‡Ä±ktÄ± klasÃ¶rÃ¼ oluÅŸturuldu: {self.output_dir}")
            
        if not os.path.exists(self.models_dir):
            os.makedirs(self.models_dir)
            print(f"ğŸ“ Model klasÃ¶rÃ¼ oluÅŸturuldu: {self.models_dir}")
        
    def prepare_data(self):
        """Veri Ã¶n iÅŸleme ve hazÄ±rlÄ±k"""
        print("\nğŸ”„ Veri Ã¶n iÅŸleme...")
        
        # Feature ve target ayrÄ±mÄ±
        feature_cols = ['Ip', 'TMS', 'IL', 'Isc', 'PWT1', 'PWT3', 'FL', 'Ftype']
        target_col = 'TIME'
        
        self.X_train = self.train_df[feature_cols]
        self.y_train = self.train_df[target_col]
        self.X_test = self.test_df[feature_cols]
        self.y_test = self.test_df[target_col]
        
        # Ftype daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
        print("\nğŸ“ˆ Ftype DaÄŸÄ±lÄ±mÄ± (Train):")
        ftype_dist = self.train_df['Ftype'].value_counts().sort_index()
        for ftype, count in ftype_dist.items():
            percentage = (count / len(self.train_df)) * 100
            print(f"   Ftype {ftype}: {count:,} adet ({percentage:.1f}%)")
        
        # Temel istatistikler
        print(f"\nğŸ“Š TIME Kolonunun Ä°statistikleri:")
        print(f"   Min: {self.y_train.min():.2f}")
        print(f"   Max: {self.y_train.max():.2f}")
        print(f"   Ortalama: {self.y_train.mean():.2f}")
        print(f"   Std: {self.y_train.std():.2f}")
        
        # One-hot encoding iÃ§in Ftype kolonunu kategorik olarak iÅŸle
        print(f"\nğŸ”§ One-hot encoding uygulanÄ±yor...")
        print(f"   Ã–nceki feature sayÄ±sÄ±: {self.X_train.shape[1]}")
        
        # One-hot encoding (drop_first=True ile multicollinearity Ã¶nlenir)
        self.X_train = pd.get_dummies(self.X_train, columns=['Ftype'], drop_first=True)
        self.X_test = pd.get_dummies(self.X_test, columns=['Ftype'], drop_first=True)
        
        print(f"   Sonraki feature sayÄ±sÄ±: {self.X_train.shape[1]}")
        print(f"   Yeni feature isimleri: {list(self.X_train.columns)}")
        
        # Normalizasyon (Neural Network iÃ§in)
        self.scaler = StandardScaler()
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print("âœ… Veri hazÄ±rlÄ±ÄŸÄ± tamamlandÄ±!")
        
    def train_linear_regression(self):
        """Linear Regression modeli eÄŸitimi"""
        print("\nğŸ¤– Linear Regression EÄŸitiliyor...")
        
        model = LinearRegression()
        model.fit(self.X_train, self.y_train)
        
        # Tahmin
        y_pred_train = model.predict(self.X_train)
        y_pred_test = model.predict(self.X_test)
        
        # Train metrikleri
        train_rmse = np.sqrt(mean_squared_error(self.y_train, y_pred_train))
        train_mae = mean_absolute_error(self.y_train, y_pred_train)
        train_r2 = r2_score(self.y_train, y_pred_train)
        
        # Test metrikleri
        test_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred_test))
        test_mae = mean_absolute_error(self.y_test, y_pred_test)
        test_r2 = r2_score(self.y_test, y_pred_test)
        
        # Cross-validation
        cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5, scoring='r2')
        
        # SonuÃ§larÄ± kaydet
        self.results['Linear Regression'] = {
            'RMSE': test_rmse,
            'MAE': test_mae,
            'RÂ²': test_r2,
            'Train_RMSE': train_rmse,
            'Train_MAE': train_mae,
            'Train_RÂ²': train_r2,
            'CV_RÂ²_mean': cv_scores.mean(),
            'CV_RÂ²_std': cv_scores.std(),
            'Overfitting_Score': train_r2 - test_r2
        }
        self.predictions['Linear Regression'] = y_pred_test
        self.models['Linear Regression'] = model
        self.validation_results['Linear Regression'] = cv_scores
        
        # Modeli kaydet
        joblib.dump(model, f'{self.models_dir}/linear_regression_model.pkl')
        
        print(f"   Train RÂ²: {train_r2:.4f}, Test RÂ²: {test_r2:.4f}")
        print(f"   RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}")
        print(f"   CV RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        print(f"   Overfitting: {train_r2 - test_r2:.4f}")
        
    def train_xgboost(self):
        """XGBoost modeli eÄŸitimi"""
        print("\nğŸš€ XGBoost EÄŸitiliyor...")
        
        model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            n_jobs=-1
        )
        
        model.fit(self.X_train, self.y_train)
        
        # Tahmin
        y_pred_train = model.predict(self.X_train)
        y_pred_test = model.predict(self.X_test)
        
        # Train metrikleri
        train_rmse = np.sqrt(mean_squared_error(self.y_train, y_pred_train))
        train_mae = mean_absolute_error(self.y_train, y_pred_train)
        train_r2 = r2_score(self.y_train, y_pred_train)
        
        # Test metrikleri
        test_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred_test))
        test_mae = mean_absolute_error(self.y_test, y_pred_test)
        test_r2 = r2_score(self.y_test, y_pred_test)
        
        # Cross-validation
        cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5, scoring='r2')
        
        # SonuÃ§larÄ± kaydet
        self.results['XGBoost'] = {
            'RMSE': test_rmse,
            'MAE': test_mae,
            'RÂ²': test_r2,
            'Train_RMSE': train_rmse,
            'Train_MAE': train_mae,
            'Train_RÂ²': train_r2,
            'CV_RÂ²_mean': cv_scores.mean(),
            'CV_RÂ²_std': cv_scores.std(),
            'Overfitting_Score': train_r2 - test_r2
        }
        self.predictions['XGBoost'] = y_pred_test
        self.models['XGBoost'] = model
        self.validation_results['XGBoost'] = cv_scores
        
        # Modeli kaydet
        joblib.dump(model, f'{self.models_dir}/xgboost_model.pkl')
        
        print(f"   Train RÂ²: {train_r2:.4f}, Test RÂ²: {test_r2:.4f}")
        print(f"   RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}")
        print(f"   CV RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        print(f"   Overfitting: {train_r2 - test_r2:.4f}")
        
    def train_lightgbm(self):
        """LightGBM modeli eÄŸitimi"""
        print("\nâš¡ LightGBM EÄŸitiliyor...")
        
        model = lgb.LGBMRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            n_jobs=-1,
            verbose=-1  # Sessiz mod
        )
        
        model.fit(self.X_train, self.y_train)
        
        # Tahmin
        y_pred_train = model.predict(self.X_train)
        y_pred_test = model.predict(self.X_test)
        
        # Train metrikleri
        train_rmse = np.sqrt(mean_squared_error(self.y_train, y_pred_train))
        train_mae = mean_absolute_error(self.y_train, y_pred_train)
        train_r2 = r2_score(self.y_train, y_pred_train)
        
        # Test metrikleri
        test_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred_test))
        test_mae = mean_absolute_error(self.y_test, y_pred_test)
        test_r2 = r2_score(self.y_test, y_pred_test)
        
        # Cross-validation
        cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5, scoring='r2')
        
        # SonuÃ§larÄ± kaydet
        self.results['LightGBM'] = {
            'RMSE': test_rmse,
            'MAE': test_mae,
            'RÂ²': test_r2,
            'Train_RMSE': train_rmse,
            'Train_MAE': train_mae,
            'Train_RÂ²': train_r2,
            'CV_RÂ²_mean': cv_scores.mean(),
            'CV_RÂ²_std': cv_scores.std(),
            'Overfitting_Score': train_r2 - test_r2
        }
        self.predictions['LightGBM'] = y_pred_test
        self.models['LightGBM'] = model
        self.validation_results['LightGBM'] = cv_scores
        
        # Modeli kaydet
        joblib.dump(model, f'{self.models_dir}/lightgbm_model.pkl')
        
        print(f"   Train RÂ²: {train_r2:.4f}, Test RÂ²: {test_r2:.4f}")
        print(f"   RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}")
        print(f"   CV RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        print(f"   Overfitting: {train_r2 - test_r2:.4f}")
        
    def train_neural_network(self):
        """Neural Network (MLP) modeli eÄŸitimi"""
        print("\nğŸ§  Neural Network (MLP) EÄŸitiliyor...")
        
        model = MLPRegressor(
            hidden_layer_sizes=(100, 50),  # 2 gizli katman
            max_iter=300,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1,
            n_iter_no_change=10
        )
        
        # Scaled verilerle eÄŸit
        model.fit(self.X_train_scaled, self.y_train)
        
        # Tahmin
        y_pred_train = model.predict(self.X_train_scaled)
        y_pred_test = model.predict(self.X_test_scaled)
        
        # Train metrikleri
        train_rmse = np.sqrt(mean_squared_error(self.y_train, y_pred_train))
        train_mae = mean_absolute_error(self.y_train, y_pred_train)
        train_r2 = r2_score(self.y_train, y_pred_train)
        
        # Test metrikleri
        test_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred_test))
        test_mae = mean_absolute_error(self.y_test, y_pred_test)
        test_r2 = r2_score(self.y_test, y_pred_test)
        
        # Cross-validation (scaled verilerle)
        cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=5, scoring='r2')
        
        # SonuÃ§larÄ± kaydet
        self.results['Neural Network'] = {
            'RMSE': test_rmse,
            'MAE': test_mae,
            'RÂ²': test_r2,
            'Train_RMSE': train_rmse,
            'Train_MAE': train_mae,
            'Train_RÂ²': train_r2,
            'CV_RÂ²_mean': cv_scores.mean(),
            'CV_RÂ²_std': cv_scores.std(),
            'Overfitting_Score': train_r2 - test_r2
        }
        self.predictions['Neural Network'] = y_pred_test
        self.models['Neural Network'] = model
        self.validation_results['Neural Network'] = cv_scores
        
        # Modeli ve scaler'Ä± kaydet
        joblib.dump(model, f'{self.models_dir}/neural_network_model.pkl')
        joblib.dump(self.scaler, f'{self.models_dir}/scaler.pkl')
        
        print(f"   Train RÂ²: {train_r2:.4f}, Test RÂ²: {test_r2:.4f}")
        print(f"   RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}")
        print(f"   CV RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        print(f"   Overfitting: {train_r2 - test_r2:.4f}")
        
    def train_all_models(self):
        """TÃ¼m modelleri eÄŸit"""
        print("\nğŸ¯ TÃ¼m Modeller EÄŸitiliyor...")
        start_time = datetime.now()
        
        self.train_linear_regression()
        self.train_xgboost()
        self.train_lightgbm()
        self.train_neural_network()
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        print(f"\nâ±ï¸ Toplam eÄŸitim sÃ¼resi: {duration:.2f} saniye")
        
    def compare_results(self):
        """Model sonuÃ§larÄ±nÄ± karÅŸÄ±laÅŸtÄ±r"""
        print("\n" + "=" * 60)
        print("ğŸ“Š MODEL KARÅILAÅTIRMA SONUÃ‡LARI")
        print("=" * 60)
        
        # SonuÃ§larÄ± DataFrame'e Ã§evir
        results_df = pd.DataFrame(self.results).T
        results_df = results_df[['RMSE', 'MAE', 'RÂ²']]  # SÄ±ralama
        
        # En iyi modelleri belirle
        best_rmse = results_df['RMSE'].astype(float).idxmin()
        best_mae = results_df['MAE'].astype(float).idxmin()
        best_r2 = results_df['RÂ²'].astype(float).idxmax()
        
        print("\nğŸ† En Ä°yi Performans:")
        print(f"   En DÃ¼ÅŸÃ¼k RMSE: {best_rmse} ({results_df.loc[best_rmse, 'RMSE']:.4f})")
        print(f"   En DÃ¼ÅŸÃ¼k MAE: {best_mae} ({results_df.loc[best_mae, 'MAE']:.4f})")
        print(f"   En YÃ¼ksek RÂ²: {best_r2} ({results_df.loc[best_r2, 'RÂ²']:.4f})")
        
        print("\nğŸ“‹ DetaylÄ± SonuÃ§lar:")
        print(results_df.round(4))
        
        # GÃ¶rselleÅŸtirme
        self.plot_results(results_df)
        
        # DetaylÄ± gÃ¶rselleÅŸtirmeler
        self.advanced_visualizations()
        
        # Ftype 1 + TMS 0.1 Ã¶zel analizi
        self.ftype1_tms01_analysis()
        
        # Overfitting analizi
        self.overfitting_analysis()
        
        # TMS spesifik analiz
        self.tms_specific_analysis()
        
        return results_df
        
    def plot_results(self, results_df):
        """SonuÃ§larÄ± gÃ¶rselleÅŸtir"""
        print("\nğŸ“ˆ Grafikler oluÅŸturuluyor...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=16, fontweight='bold')
        
        # 1. Metrik karÅŸÄ±laÅŸtÄ±rmasÄ±
        ax1 = axes[0, 0]
        results_df[['RMSE', 'MAE']].plot(kind='bar', ax=ax1)
        ax1.set_title('RMSE ve MAE KarÅŸÄ±laÅŸtÄ±rmasÄ±')
        ax1.set_ylabel('Hata DeÄŸeri')
        ax1.legend()
        ax1.tick_params(axis='x', rotation=45)
        
        # 2. RÂ² karÅŸÄ±laÅŸtÄ±rmasÄ±
        ax2 = axes[0, 1]
        results_df['RÂ²'].plot(kind='bar', ax=ax2, color='green', alpha=0.7)
        ax2.set_title('RÂ² Skoru KarÅŸÄ±laÅŸtÄ±rmasÄ±')
        ax2.set_ylabel('RÂ² DeÄŸeri')
        ax2.tick_params(axis='x', rotation=45)
        
        # 3. GerÃ§ek vs Tahmin (En iyi model)
        best_model = results_df['RÂ²'].astype(float).idxmax()
        ax3 = axes[1, 0]
        y_pred_best = self.predictions[best_model]
        
        # Sampling yaparak Ã§izim hÄ±zlandÄ±r (Ã§ok veri varsa)
        if len(self.y_test) > 10000:
            sample_idx = np.random.choice(len(self.y_test), 10000, replace=False)
            y_test_sample = self.y_test.iloc[sample_idx]
            y_pred_sample = y_pred_best[sample_idx]
        else:
            y_test_sample = self.y_test
            y_pred_sample = y_pred_best
            
        ax3.scatter(y_test_sample, y_pred_sample, alpha=0.5, s=1)
        ax3.plot([y_test_sample.min(), y_test_sample.max()], 
                [y_test_sample.min(), y_test_sample.max()], 'r--', lw=2)
        ax3.set_xlabel('GerÃ§ek DeÄŸerler')
        ax3.set_ylabel('Tahmin Edilen DeÄŸerler')
        ax3.set_title(f'GerÃ§ek vs Tahmin - {best_model}')
        
        # 4. Hata daÄŸÄ±lÄ±mÄ±
        ax4 = axes[1, 1]
        residuals = self.y_test - y_pred_best
        if len(residuals) > 10000:
            residuals_sample = residuals.iloc[sample_idx]
        else:
            residuals_sample = residuals
            
        ax4.hist(residuals_sample, bins=50, alpha=0.7, edgecolor='black')
        ax4.set_xlabel('Hata (GerÃ§ek - Tahmin)')
        ax4.set_ylabel('Frekans')
        ax4.set_title(f'Hata DaÄŸÄ±lÄ±mÄ± - {best_model}')
        ax4.axvline(x=0, color='red', linestyle='--')
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/model_comparison_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… Grafikler '{self.output_dir}/model_comparison_results.png' olarak kaydedildi!")
        
    def advanced_visualizations(self):
        """DetaylÄ± gÃ¶rselleÅŸtirmeler - 6 farklÄ± analiz"""
        print("\nğŸ“ˆ DetaylÄ± gÃ¶rselleÅŸtirmeler oluÅŸturuluyor...")
        
        # 1. GerÃ§ek vs Tahmin Scatter Plots
        self.plot_actual_vs_predicted()
        
        # 2. Hata DaÄŸÄ±lÄ±mÄ± (Residual Plots)
        self.plot_residual_analysis()
        
        # 3. KapsamlÄ± Feature Importance (TÃ¼m modeller iÃ§in)
        self.plot_comprehensive_feature_importance()
        
        # 4. Train vs Test Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±
        self.plot_train_vs_test_performance()
        
        # 5. TIME DeÄŸerlerinin DaÄŸÄ±lÄ±mÄ±
        self.plot_time_distribution()
        
        # 6. Feature vs TIME Ä°liÅŸkiler
        self.plot_feature_vs_time_relationships()
        
    def plot_actual_vs_predicted(self):
        """1ï¸âƒ£ GerÃ§ek vs Tahmin Scatter Plots"""
        print("   ğŸ“Š 1ï¸âƒ£ GerÃ§ek vs Tahmin grafiÄŸi...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('GerÃ§ek vs Tahmin DeÄŸerleri - TÃ¼m Modeller', fontsize=16, fontweight='bold')
        
        models = list(self.predictions.keys())
        
        for i, model_name in enumerate(models):
            row, col = i // 2, i % 2
            ax = axes[row, col]
            
            y_pred = self.predictions[model_name]
            y_true = self.y_test
            
            # Sample data if too large
            if len(y_true) > 10000:
                sample_idx = np.random.choice(len(y_true), 10000, replace=False)
                y_true_sample = y_true.iloc[sample_idx]
                y_pred_sample = y_pred[sample_idx]
            else:
                y_true_sample = y_true
                y_pred_sample = y_pred
            
            # Scatter plot
            ax.scatter(y_true_sample, y_pred_sample, alpha=0.6, s=1, color='blue')
            
            # Perfect prediction line (y=x)
            min_val = min(y_true_sample.min(), y_pred_sample.min())
            max_val = max(y_true_sample.max(), y_pred_sample.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')
            
            # Metrics
            r2 = self.results[model_name]['RÂ²']
            rmse = self.results[model_name]['RMSE']
            
            ax.set_xlabel('GerÃ§ek TIME')
            ax.set_ylabel('Tahmin TIME')
            ax.set_title(f'{model_name}\\nRÂ² = {r2:.4f}, RMSE = {rmse:.4f}')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # Add correlation line
            from scipy import stats
            slope, intercept, r_value, p_value, std_err = stats.linregress(y_true_sample, y_pred_sample)
            line = slope * y_true_sample + intercept
            ax.plot(y_true_sample.sort_values(), 
                   slope * y_true_sample.sort_values() + intercept, 
                   'g-', alpha=0.8, label=f'Trend (r={r_value:.3f})')
            ax.legend()
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/actual_vs_predicted.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def plot_residual_analysis(self):
        """2ï¸âƒ£ Hata DaÄŸÄ±lÄ±mÄ± (Residual Analysis)"""
        print("   ğŸ“Š 2ï¸âƒ£ Hata daÄŸÄ±lÄ±mÄ± analizi...")
        
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        fig.suptitle('Hata DaÄŸÄ±lÄ±mÄ± Analizi (Residuals)', fontsize=16, fontweight='bold')
        
        models = list(self.predictions.keys())
        
        for i, model_name in enumerate(models):
            # Residuals vs Predicted
            ax1 = axes[0, i]
            y_pred = self.predictions[model_name]
            y_true = self.y_test
            residuals = y_true - y_pred
            
            # Sample for plotting
            if len(residuals) > 5000:
                sample_idx = np.random.choice(len(residuals), 5000, replace=False)
                y_pred_sample = y_pred[sample_idx]
                residuals_sample = residuals.iloc[sample_idx]
            else:
                y_pred_sample = y_pred
                residuals_sample = residuals
            
            ax1.scatter(y_pred_sample, residuals_sample, alpha=0.6, s=1)
            ax1.axhline(y=0, color='red', linestyle='--', alpha=0.8)
            ax1.set_xlabel('Tahmin DeÄŸerleri')
            ax1.set_ylabel('Residuals (GerÃ§ek - Tahmin)')
            ax1.set_title(f'{model_name}\\nResiduals vs Predicted')
            ax1.grid(True, alpha=0.3)
            
            # Residuals distribution
            ax2 = axes[1, i]
            ax2.hist(residuals_sample, bins=50, alpha=0.7, edgecolor='black', density=True)
            ax2.axvline(x=0, color='red', linestyle='--', alpha=0.8)
            ax2.axvline(x=residuals.mean(), color='green', linestyle='-', alpha=0.8, 
                       label=f'Mean: {residuals.mean():.4f}')
            ax2.set_xlabel('Residuals')
            ax2.set_ylabel('YoÄŸunluk')
            ax2.set_title(f'{model_name}\\nResiduals DaÄŸÄ±lÄ±mÄ±')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # Add statistics
            rmse = np.sqrt(np.mean(residuals**2))
            mae = np.mean(np.abs(residuals))
            std = residuals.std()
            ax2.text(0.05, 0.95, f'RMSE: {rmse:.4f}\\nMAE: {mae:.4f}\\nStd: {std:.4f}', 
                    transform=ax2.transAxes, verticalalignment='top',
                    bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/residual_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def plot_comprehensive_feature_importance(self):
        """3ï¸âƒ£ KapsamlÄ± Feature Importance (TÃ¼m modeller iÃ§in)"""
        print("   ğŸ“Š 3ï¸âƒ£ KapsamlÄ± feature importance analizi...")
        
        fig, axes = plt.subplots(2, 2, figsize=(20, 12))
        fig.suptitle('Feature Importance - TÃ¼m Modeller', fontsize=16, fontweight='bold')
        
        feature_names = list(self.X_train.columns)
        
        # 1. XGBoost Feature Importance
        if 'XGBoost' in self.models:
            ax1 = axes[0, 0]
            importance = self.models['XGBoost'].feature_importances_
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importance
            }).sort_values('importance', ascending=True)
            
            ax1.barh(importance_df['feature'], importance_df['importance'], color='skyblue', alpha=0.8)
            ax1.set_title('XGBoost Feature Importance')
            ax1.set_xlabel('Importance Score')
            ax1.grid(True, alpha=0.3)
        
        # 2. LightGBM Feature Importance
        if 'LightGBM' in self.models:
            ax2 = axes[0, 1]
            importance = self.models['LightGBM'].feature_importances_
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importance
            }).sort_values('importance', ascending=True)
            
            ax2.barh(importance_df['feature'], importance_df['importance'], color='lightgreen', alpha=0.8)
            ax2.set_title('LightGBM Feature Importance')
            ax2.set_xlabel('Importance Score')
            ax2.grid(True, alpha=0.3)
        
        # 3. Linear Regression Coefficients (as importance)
        if 'Linear Regression' in self.models:
            ax3 = axes[1, 0]
            coefs = np.abs(self.models['Linear Regression'].coef_)
            coef_df = pd.DataFrame({
                'feature': feature_names,
                'importance': coefs
            }).sort_values('importance', ascending=True)
            
            ax3.barh(coef_df['feature'], coef_df['importance'], color='orange', alpha=0.8)
            ax3.set_title('Linear Regression |Coefficients|')
            ax3.set_xlabel('|Coefficient| Value')
            ax3.grid(True, alpha=0.3)
        
        # 4. Correlation-based importance
        ax4 = axes[1, 1]
        correlations = []
        for feature in feature_names:
            corr = abs(self.X_train[feature].corr(self.y_train))
            correlations.append(corr)
        
        corr_df = pd.DataFrame({
            'feature': feature_names,
            'importance': correlations
        }).sort_values('importance', ascending=True)
        
        ax4.barh(corr_df['feature'], corr_df['importance'], color='purple', alpha=0.8)
        ax4.set_title('Feature-TIME |Correlation|')
        ax4.set_xlabel('|Correlation| Value')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/comprehensive_feature_importance.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def plot_train_vs_test_performance(self):
        """4ï¸âƒ£ Train vs Test Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±"""
        print("   ğŸ“Š 4ï¸âƒ£ Train vs Test performans karÅŸÄ±laÅŸtÄ±rmasÄ±...")
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        fig.suptitle('Train vs Test Performans KarÅŸÄ±laÅŸtÄ±rmasÄ± (Overfitting KontrolÃ¼)', fontsize=16)
        
        models = list(self.results.keys())
        train_r2 = [self.results[m]['Train_RÂ²'] for m in models]
        test_r2 = [self.results[m]['RÂ²'] for m in models]
        cv_r2 = [self.results[m]['CV_RÂ²_mean'] for m in models]
        overfitting_scores = [self.results[m]['Overfitting_Score'] for m in models]
        
        # 1. RÂ² Comparison
        ax1 = axes[0]
        x = np.arange(len(models))
        width = 0.25
        
        ax1.bar(x - width, train_r2, width, label='Train RÂ²', alpha=0.8, color='blue')
        ax1.bar(x, test_r2, width, label='Test RÂ²', alpha=0.8, color='orange')
        ax1.bar(x + width, cv_r2, width, label='CV RÂ²', alpha=0.8, color='green')
        
        ax1.set_xlabel('Modeller')
        ax1.set_ylabel('RÂ² Score')
        ax1.set_title('RÂ² SkorlarÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±')
        ax1.set_xticks(x)
        ax1.set_xticklabels(models, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Add values on bars
        for i, (train, test, cv) in enumerate(zip(train_r2, test_r2, cv_r2)):
            ax1.text(i - width, train + 0.001, f'{train:.4f}', ha='center', va='bottom', fontsize=8)
            ax1.text(i, test + 0.001, f'{test:.4f}', ha='center', va='bottom', fontsize=8)
            ax1.text(i + width, cv + 0.001, f'{cv:.4f}', ha='center', va='bottom', fontsize=8)
        
        # 2. Overfitting Scores
        ax2 = axes[1]
        colors = ['green' if abs(score) < 0.01 else 'orange' if abs(score) < 0.05 else 'red' 
                 for score in overfitting_scores]
        bars = ax2.bar(models, overfitting_scores, color=colors, alpha=0.7)
        
        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)
        ax2.axhline(y=0.01, color='red', linestyle='--', alpha=0.5, label='Overfitting Threshold')
        ax2.axhline(y=-0.01, color='red', linestyle='--', alpha=0.5)
        
        ax2.set_xlabel('Modeller')
        ax2.set_ylabel('Overfitting Score (Train RÂ² - Test RÂ²)')
        ax2.set_title('Overfitting SkorlarÄ±')
        ax2.tick_params(axis='x', rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Add values on bars
        for bar, score in zip(bars, overfitting_scores):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + (0.0005 if height >= 0 else -0.0005),
                    f'{score:+.4f}', ha='center', va='bottom' if height >= 0 else 'top', fontsize=10)
        
        # 3. Performance Summary Table
        ax3 = axes[2]
        ax3.axis('tight')
        ax3.axis('off')
        
        table_data = []
        for model in models:
            overfitting_status = "âœ… YOK" if abs(self.results[model]['Overfitting_Score']) < 0.01 else "âš ï¸ VAR"
            table_data.append([
                model,
                f"{self.results[model]['RÂ²']:.4f}",
                f"{self.results[model]['RMSE']:.4f}",
                f"{self.results[model]['Overfitting_Score']:+.4f}",
                overfitting_status
            ])
        
        table = ax3.table(cellText=table_data,
                         colLabels=['Model', 'Test RÂ²', 'RMSE', 'Overfitting', 'Status'],
                         cellLoc='center',
                         loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 1.5)
        ax3.set_title('Performans Ã–zeti', pad=20)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/train_vs_test_performance.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def plot_time_distribution(self):
        """5ï¸âƒ£ TIME DeÄŸerlerinin DaÄŸÄ±lÄ±mÄ±"""
        print("   ğŸ“Š 5ï¸âƒ£ TIME daÄŸÄ±lÄ±mÄ± analizi...")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('TIME DeÄŸerlerinin DaÄŸÄ±lÄ±m Analizi', fontsize=16, fontweight='bold')
        
        # 1. Overall TIME distribution
        ax1 = axes[0, 0]
        ax1.hist(self.y_train, bins=50, alpha=0.7, color='skyblue', edgecolor='black', density=True, label='Train')
        ax1.hist(self.y_test, bins=50, alpha=0.7, color='orange', edgecolor='black', density=True, label='Test')
        ax1.set_xlabel('TIME DeÄŸeri')
        ax1.set_ylabel('YoÄŸunluk')
        ax1.set_title('Genel TIME DaÄŸÄ±lÄ±mÄ±')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. TIME by Ftype
        ax2 = axes[0, 1]
        for ftype in sorted(self.train_df['Ftype'].unique()):
            ftype_data = self.train_df[self.train_df['Ftype'] == ftype]['TIME']
            ax2.hist(ftype_data, bins=30, alpha=0.6, label=f'Ftype {ftype}', density=True)
        ax2.set_xlabel('TIME DeÄŸeri')
        ax2.set_ylabel('YoÄŸunluk')
        ax2.set_title('Ftype\'a GÃ¶re TIME DaÄŸÄ±lÄ±mÄ±')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. TIME statistics by Ftype
        ax3 = axes[0, 2]
        ftype_stats = []
        for ftype in sorted(self.train_df['Ftype'].unique()):
            ftype_data = self.train_df[self.train_df['Ftype'] == ftype]['TIME']
            ftype_stats.append([
                f'Ftype {ftype}',
                f'{ftype_data.mean():.4f}',
                f'{ftype_data.std():.4f}',
                f'{ftype_data.min():.4f}',
                f'{ftype_data.max():.4f}',
                f'{len(ftype_data):,}'
            ])
        
        table = ax3.table(cellText=ftype_stats,
                         colLabels=['Ftype', 'Mean', 'Std', 'Min', 'Max', 'Count'],
                         cellLoc='center',
                         loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1.2, 1.5)
        ax3.axis('off')
        ax3.set_title('Ftype Ä°statistikleri')
        
        # 4. TIME vs TMS relationship
        ax4 = axes[1, 0]
        sample_size = min(5000, len(self.train_df))
        sample_df = self.train_df.sample(sample_size)
        
        scatter = ax4.scatter(sample_df['TMS'], sample_df['TIME'], 
                            c=sample_df['Ftype'], cmap='viridis', alpha=0.6, s=1)
        ax4.set_xlabel('TMS')
        ax4.set_ylabel('TIME')
        ax4.set_title('TMS vs TIME Ä°liÅŸkisi')
        plt.colorbar(scatter, ax=ax4, label='Ftype')
        ax4.grid(True, alpha=0.3)
        
        # 5. Prediction accuracy by TIME ranges
        ax5 = axes[1, 1]
        best_model = max(self.results.keys(), key=lambda x: self.results[x]['RÂ²'])
        y_pred_best = self.predictions[best_model]
        
        # Divide TIME into ranges
        time_ranges = pd.cut(self.y_test, bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
        accuracy_by_range = []
        
        for range_name in ['Very Low', 'Low', 'Medium', 'High', 'Very High']:
            mask = (time_ranges == range_name)
            if mask.sum() > 0:
                range_r2 = r2_score(self.y_test[mask], y_pred_best[mask])
                range_rmse = np.sqrt(mean_squared_error(self.y_test[mask], y_pred_best[mask]))
                accuracy_by_range.append([range_name, f'{range_r2:.4f}', f'{range_rmse:.4f}', f'{mask.sum():,}'])
        
        table2 = ax5.table(cellText=accuracy_by_range,
                          colLabels=['TIME Range', 'RÂ²', 'RMSE', 'Count'],
                          cellLoc='center',
                          loc='center')
        table2.auto_set_font_size(False)
        table2.set_fontsize(10)
        table2.scale(1.2, 1.5)
        ax5.axis('off')
        ax5.set_title(f'{best_model} - TIME AralÄ±klarÄ±na GÃ¶re Performans')
        
        # 6. Cumulative distribution
        ax6 = axes[1, 2]
        ax6.hist(self.y_train, bins=100, cumulative=True, density=True, alpha=0.7, 
                label='Train CDF', histtype='step', linewidth=2)
        ax6.hist(self.y_test, bins=100, cumulative=True, density=True, alpha=0.7, 
                label='Test CDF', histtype='step', linewidth=2)
        ax6.set_xlabel('TIME DeÄŸeri')
        ax6.set_ylabel('KÃ¼mÃ¼latif OlasÄ±lÄ±k')
        ax6.set_title('KÃ¼mÃ¼latif DaÄŸÄ±lÄ±m Fonksiyonu')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/time_distribution.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def plot_feature_vs_time_relationships(self):
        """6ï¸âƒ£ Feature vs TIME Ä°liÅŸkiler"""
        print("   ğŸ“Š 6ï¸âƒ£ Feature vs TIME iliÅŸki analizi...")
        
        # En Ã¶nemli feature'larÄ± belirle (kategorik sÃ¼tunlarÄ± kontrol et)
        feature_names = list(self.X_train.columns)
        print(f"     Toplam feature sayÄ±sÄ±: {len(feature_names)}")
        print(f"     Ã–rnek feature'lar: {feature_names[:10]}")
        
        correlations = []
        for feature in feature_names:
            try:
                corr = abs(self.X_train[feature].corr(self.y_train))
                if not np.isnan(corr):
                    correlations.append((feature, corr))
            except Exception as e:
                print(f"     âš ï¸  {feature} feature'Ä± iÃ§in korelasyon hesaplanamadÄ±: {e}")
                
        correlations.sort(key=lambda x: x[1], reverse=True)
        top_features = [f[0] for f in correlations[:6]]  # En Ã¶nemli 6 feature
        
        print(f"     En Ã¶nemli 6 feature: {top_features}")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('En Ã–nemli Feature\'lar vs TIME Ä°liÅŸkileri', fontsize=16, fontweight='bold')
        
        sample_size = min(5000, len(self.train_df))
        sample_df = self.train_df.sample(sample_size)
        
        for i, feature in enumerate(top_features):
            row, col = i // 3, i % 3
            ax = axes[row, col]
            
            try:
                # Scatter plot with Ftype coloring
                scatter = ax.scatter(sample_df[feature], sample_df['TIME'], 
                                   c=sample_df['Ftype'], cmap='viridis', alpha=0.6, s=15)
                
                # Correlation line
                from scipy import stats
                slope, intercept, r_value, p_value, std_err = stats.linregress(sample_df[feature], sample_df['TIME'])
                line_x = np.array([sample_df[feature].min(), sample_df[feature].max()])
                line_y = slope * line_x + intercept
                ax.plot(line_x, line_y, 'r-', alpha=0.8, linewidth=2, label=f'r = {r_value:.3f}')
                
                ax.set_xlabel(feature)
                ax.set_ylabel('TIME')
                ax.set_title(f'{feature} vs TIME\\n(Korelasyon: {correlations[i][1]:.4f})')
                ax.legend()
                ax.grid(True, alpha=0.3)
                
                # Add colorbar for the first subplot
                if i == 0:
                    plt.colorbar(scatter, ax=ax, label='Ftype')
                    
            except Exception as e:
                print(f"     âš ï¸  {feature} grafiÄŸi oluÅŸturulamadÄ±: {e}")
                ax.text(0.5, 0.5, f'Grafik oluÅŸturulamadÄ±\\n{feature}', 
                       ha='center', va='center', transform=ax.transAxes)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/feature_vs_time_relationships.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Correlation matrix heatmap
        try:
            fig, ax = plt.subplots(1, 1, figsize=(12, 10))
            
            # Create correlation matrix
            corr_data = self.X_train.copy()
            corr_data['TIME'] = self.y_train
            correlation_matrix = corr_data.corr()
            
            # Plot heatmap
            mask = np.triu(np.ones_like(correlation_matrix))
            sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                       square=True, linewidths=0.5, cbar_kws={"shrink": .8}, ax=ax)
            ax.set_title('Feature Korelasyon Matrisi (TIME dahil)', fontsize=14, fontweight='bold')
            
            plt.tight_layout()
            plt.savefig(f'{self.output_dir}/correlation_heatmap.png', dpi=300, bbox_inches='tight')
            plt.show()
            
        except Exception as e:
            print(f"     âš ï¸  Korelasyon matrisi oluÅŸturulamadÄ±: {e}")
            
            # Basit korelasyon tablosu oluÅŸtur
            print("     ğŸ”„ Basit korelasyon tablosu oluÅŸturuluyor...")
            fig, ax = plt.subplots(1, 1, figsize=(10, 8))
            
            # Sadece sÃ¼rekli deÄŸiÅŸkenler iÃ§in korelasyon
            numeric_features = []
            for col in self.X_train.columns:
                if not col.startswith('Ftype_'):
                    numeric_features.append(col)
            
            if numeric_features:
                corr_data = self.X_train[numeric_features].copy()
                corr_data['TIME'] = self.y_train
                correlation_matrix = corr_data.corr()
                
                sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                           square=True, linewidths=0.5, cbar_kws={"shrink": .8}, ax=ax)
                ax.set_title('Numeric Features Korelasyon Matrisi', fontsize=14, fontweight='bold')
                
                plt.tight_layout()
                plt.savefig(f'{self.output_dir}/correlation_heatmap.png', dpi=300, bbox_inches='tight')
                plt.show()
            else:
                print("     âŒ Korelasyon matrisi iÃ§in uygun feature bulunamadÄ±")
        
    def ftype1_tms01_analysis(self):
        """Ftype 1 + TMS 0.1 iÃ§in detaylÄ± model analizi"""
        print("\nğŸ”¬ Ftype 1 + TMS 0.1 Ã–zel Analizi...")
        
        # Test verisinde Ftype=1 ve TMS=0.1 olan kayÄ±tlarÄ± filtrele
        test_condition = (self.test_df['Ftype'] == 1) & (self.test_df['TMS'] == 0.1)
        filtered_test = self.test_df[test_condition].copy()
        
        if len(filtered_test) == 0:
            print("âŒ Ftype=1 ve TMS=0.1 koÅŸulunu saÄŸlayan test verisi bulunamadÄ±!")
            return
            
        print(f"ğŸ“Š Ftype=1 & TMS=0.1 test kayÄ±t sayÄ±sÄ±: {len(filtered_test)}")
        
        # GerÃ§ek TIME deÄŸerleri
        actual_times = filtered_test['TIME'].values
        actual_min, actual_max = actual_times.min(), actual_times.max()
        actual_mean, actual_std = actual_times.mean(), actual_times.std()
        
        print(f"ğŸ“ˆ GerÃ§ek TIME deÄŸerleri:")
        print(f"   Min: {actual_min:.4f}, Max: {actual_max:.4f}")
        print(f"   Ortalama: {actual_mean:.4f} Â± {actual_std:.4f}")
        
        # X verilerini hazÄ±rla
        X_filtered = filtered_test.drop(['TIME', 'Ftype'], axis=1)
        
        # One-hot encoding uygula (sadece test iÃ§in)
        X_filtered_encoded = pd.get_dummies(X_filtered, columns=[], prefix_sep='_')
        
        # Eksik sÃ¼tunlarÄ± ekle (eÄŸitim sÄ±rasÄ±nda gÃ¶rÃ¼len sÃ¼tunlar)
        for col in self.X_train.columns:
            if col not in X_filtered_encoded.columns:
                X_filtered_encoded[col] = 0
                
        # SÃ¼tun sÄ±rasÄ±nÄ± eÄŸitim verisi ile aynÄ± yap
        X_filtered_encoded = X_filtered_encoded[self.X_train.columns]
        
        # Her model iÃ§in tahmin yap
        model_predictions = {}
        model_stats = {}
        
        for model_name, model in self.models.items():
            if model_name == 'Neural Network':
                # Sadece Neural Network iÃ§in scaling gerekli
                X_scaled = self.scaler.transform(X_filtered_encoded)
                predictions = model.predict(X_scaled)
            else:
                # Linear Regression, XGBoost, LightGBM iÃ§in scaling gerekmez
                # (Linear Regression zaten scaled data ile eÄŸitilmiÅŸ ama raw data bekliyor)
                predictions = model.predict(X_filtered_encoded)
                
            model_predictions[model_name] = predictions
            model_stats[model_name] = {
                'min': predictions.min(),
                'max': predictions.max(), 
                'mean': predictions.mean(),
                'std': predictions.std(),
                'rmse': np.sqrt(np.mean((predictions - actual_times)**2)),
                'mae': np.mean(np.abs(predictions - actual_times))
            }
            
            print(f"ğŸ¤– {model_name}:")
            print(f"   Tahmin Min: {predictions.min():.4f}, Max: {predictions.max():.4f}")
            print(f"   Tahmin Ortalama: {predictions.mean():.4f} Â± {predictions.std():.4f}")
            print(f"   RMSE: {model_stats[model_name]['rmse']:.4f}, MAE: {model_stats[model_name]['mae']:.4f}")
        
        # GÃ¶rselleÅŸtirme
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Ftype=1 & TMS=0.1 iÃ§in Model Tahminleri\n(TIME DeÄŸeri Analizi)', 
                     fontsize=16, fontweight='bold')
        
        colors = ['blue', 'orange', 'green', 'red']
        model_names = list(self.models.keys())
        
        # 1. Min-Max AralÄ±k KarÅŸÄ±laÅŸtÄ±rmasÄ± (Bar Chart)
        ax1 = axes[0, 0]
        x_pos = np.arange(len(model_names))
        mins = [model_stats[name]['min'] for name in model_names]
        maxs = [model_stats[name]['max'] for name in model_names]
        ranges = [maxs[i] - mins[i] for i in range(len(model_names))]
        
        bars = ax1.bar(x_pos, ranges, bottom=mins, alpha=0.7, color=colors)
        
        # GerÃ§ek deÄŸer aralÄ±ÄŸÄ±nÄ± Ã§iz
        ax1.axhline(y=actual_min, color='black', linestyle='--', alpha=0.8, label=f'GerÃ§ek Min: {actual_min:.4f}')
        ax1.axhline(y=actual_max, color='black', linestyle='-', alpha=0.8, label=f'GerÃ§ek Max: {actual_max:.4f}')
        
        ax1.set_xlabel('Modeller')
        ax1.set_ylabel('TIME DeÄŸeri')
        ax1.set_title('Min-Max Tahmin AralÄ±klarÄ±')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(model_names, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # DeÄŸerleri bar'larÄ±n Ã¼zerine yaz
        for i, (bar, model_name) in enumerate(zip(bars, model_names)):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., bar.get_y() + height/2,
                    f'[{mins[i]:.3f},\n{maxs[i]:.3f}]',
                    ha='center', va='center', fontweight='bold', fontsize=8)
        
        # 2. Tahmin DaÄŸÄ±lÄ±mlarÄ± (Histogram)
        ax2 = axes[0, 1]
        for i, (model_name, predictions) in enumerate(model_predictions.items()):
            ax2.hist(predictions, bins=30, alpha=0.6, color=colors[i], 
                    label=f'{model_name}', density=True)
        
        ax2.hist(actual_times, bins=30, alpha=0.8, color='black', 
                label='GerÃ§ek DeÄŸerler', density=True, histtype='step', linewidth=2)
        
        ax2.set_xlabel('TIME DeÄŸeri')
        ax2.set_ylabel('YoÄŸunluk')
        ax2.set_title('Tahmin DaÄŸÄ±lÄ±mlarÄ±')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. RMSE ve MAE KarÅŸÄ±laÅŸtÄ±rmasÄ±
        ax3 = axes[1, 0]
        rmse_values = [model_stats[name]['rmse'] for name in model_names]
        mae_values = [model_stats[name]['mae'] for name in model_names]
        
        x = np.arange(len(model_names))
        width = 0.35
        
        bars1 = ax3.bar(x - width/2, rmse_values, width, label='RMSE', alpha=0.8, color='skyblue')
        bars2 = ax3.bar(x + width/2, mae_values, width, label='MAE', alpha=0.8, color='lightcoral')
        
        ax3.set_xlabel('Modeller')
        ax3.set_ylabel('Hata DeÄŸeri')
        ax3.set_title('Model Hata Metrikleri')
        ax3.set_xticks(x)
        ax3.set_xticklabels(model_names, rotation=45)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # DeÄŸerleri bar'larÄ±n Ã¼zerine yaz
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.4f}',
                        ha='center', va='bottom', fontsize=8)
        
        # 4. GerÃ§ek vs Tahmin Scatter Plot
        ax4 = axes[1, 1]
        for i, (model_name, predictions) in enumerate(model_predictions.items()):
            ax4.scatter(actual_times, predictions, alpha=0.6, color=colors[i], 
                       label=f'{model_name}', s=20)
        
        # Perfect prediction line
        min_val = min(actual_times.min(), min([pred.min() for pred in model_predictions.values()]))
        max_val = max(actual_times.max(), max([pred.max() for pred in model_predictions.values()]))
        ax4.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8, label='MÃ¼kemmel Tahmin')
        
        ax4.set_xlabel('GerÃ§ek TIME')
        ax4.set_ylabel('Tahmin Edilen TIME')
        ax4.set_title('GerÃ§ek vs Tahmin')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/ftype1_tms01_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Ã–zet tablosu yazdÄ±r
        print(f"\nğŸ“‹ Ã–ZET TABLO (Ftype=1 & TMS=0.1):")
        print("="*80)
        print(f"{'Model':<15} {'Min':>8} {'Max':>8} {'AralÄ±k':>8} {'RMSE':>8} {'MAE':>8}")
        print("="*80)
        print(f"{'GerÃ§ek':<15} {actual_min:>8.4f} {actual_max:>8.4f} {actual_max-actual_min:>8.4f} {'-':>8} {'-':>8}")
        print("-"*80)
        for model_name in model_names:
            stats = model_stats[model_name]
            print(f"{model_name:<15} {stats['min']:>8.4f} {stats['max']:>8.4f} {stats['max']-stats['min']:>8.4f} {stats['rmse']:>8.4f} {stats['mae']:>8.4f}")
        print("="*80)
        
        return model_stats
        
    def feature_importance_analysis(self):
        """Feature importance analizi (XGBoost ve LightGBM iÃ§in)"""
        print("\nğŸ” Feature Importance Analizi...")
        
        # One-hot encoding sonrasÄ± feature isimleri
        feature_names = list(self.X_train.columns)
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # XGBoost feature importance
        if 'XGBoost' in self.models:
            xgb_model = self.models['XGBoost']
            xgb_importance = xgb_model.feature_importances_
            
            ax1 = axes[0]
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': xgb_importance
            }).sort_values('importance', ascending=True)
            
            # En Ã¶nemli 10 feature'Ä± gÃ¶ster (Ã§ok fazla varsa)
            if len(importance_df) > 10:
                importance_df = importance_df.tail(10)
            
            ax1.barh(importance_df['feature'], importance_df['importance'])
            ax1.set_title('XGBoost Feature Importance')
            ax1.set_xlabel('Importance Score')
        
        # LightGBM feature importance
        if 'LightGBM' in self.models:
            lgb_model = self.models['LightGBM']
            lgb_importance = lgb_model.feature_importances_
            
            ax2 = axes[1]
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': lgb_importance
            }).sort_values('importance', ascending=True)
            
            # En Ã¶nemli 10 feature'Ä± gÃ¶ster (Ã§ok fazla varsa)
            if len(importance_df) > 10:
                importance_df = importance_df.tail(10)
            
            ax2.barh(importance_df['feature'], importance_df['importance'])
            ax2.set_title('LightGBM Feature Importance')
            ax2.set_xlabel('Importance Score')
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/feature_importance.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… Feature importance grafikleri '{self.output_dir}/feature_importance.png' olarak kaydedildi!")
        
    def overfitting_analysis(self):
        """DetaylÄ± overfitting analizi"""
        print("\n" + "=" * 60)
        print("ğŸ”¬ OVERFITTING ANALÄ°ZÄ°")
        print("=" * 60)
        
        # Overfitting skorlarÄ±
        print("\nğŸ“Š Overfitting SkorlarÄ± (Train RÂ² - Test RÂ²):")
        overfitting_data = []
        for model_name in self.results.keys():
            overfitting_score = self.results[model_name]['Overfitting_Score']
            cv_mean = self.results[model_name]['CV_RÂ²_mean']
            cv_std = self.results[model_name]['CV_RÂ²_std']
            test_r2 = self.results[model_name]['RÂ²']
            
            print(f"   {model_name:15s}: {overfitting_score:+.4f}")
            print(f"   {'':15s}  CV RÂ²: {cv_mean:.4f} Â± {cv_std:.4f}")
            print(f"   {'':15s}  CV vs Test: {cv_mean - test_r2:+.4f}")
            print()
            
            overfitting_data.append({
                'Model': model_name,
                'Overfitting_Score': overfitting_score,
                'CV_Mean': cv_mean,
                'CV_Std': cv_std,
                'Test_R2': test_r2,
                'CV_vs_Test': cv_mean - test_r2
            })
        
        # Overfitting deÄŸerlendirmesi
        print("ğŸ¯ Overfitting DeÄŸerlendirmesi:")
        for data in overfitting_data:
            if abs(data['Overfitting_Score']) < 0.01 and abs(data['CV_vs_Test']) < 0.01:
                status = "âœ… Overfitting YOK"
            elif abs(data['Overfitting_Score']) < 0.05:
                status = "âš ï¸  Hafif Overfitting"
            else:
                status = "âŒ Ciddi Overfitting"
            print(f"   {data['Model']:15s}: {status}")
        
        # GÃ¶rselleÅŸtirme
        self.plot_overfitting_analysis(overfitting_data)
        
    def plot_overfitting_analysis(self, overfitting_data):
        """Overfitting analizi grafiÄŸi"""
        df = pd.DataFrame(overfitting_data)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Overfitting Analizi', fontsize=16, fontweight='bold')
        
        # 1. Train vs Test RÂ²
        ax1 = axes[0, 0]
        models = df['Model']
        train_r2 = [self.results[m]['Train_RÂ²'] for m in models]
        test_r2 = df['Test_R2']
        
        x = np.arange(len(models))
        width = 0.35
        ax1.bar(x - width/2, train_r2, width, label='Train RÂ²', alpha=0.8)
        ax1.bar(x + width/2, test_r2, width, label='Test RÂ²', alpha=0.8)
        ax1.set_xlabel('Modeller')
        ax1.set_ylabel('RÂ² Skoru')
        ax1.set_title('Train vs Test RÂ² KarÅŸÄ±laÅŸtÄ±rmasÄ±')
        ax1.set_xticks(x)
        ax1.set_xticklabels(models, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Cross-validation sonuÃ§larÄ±
        ax2 = axes[0, 1]
        for i, model in enumerate(models):
            cv_scores = self.validation_results[model]
            ax2.boxplot(cv_scores, positions=[i], widths=0.6)
        ax2.set_xlabel('Modeller')
        ax2.set_ylabel('CV RÂ² SkorlarÄ±')
        ax2.set_title('Cross-Validation RÂ² DaÄŸÄ±lÄ±mÄ±')
        ax2.set_xticks(range(len(models)))
        ax2.set_xticklabels(models, rotation=45)
        ax2.grid(True, alpha=0.3)
        
        # 3. Overfitting skorlarÄ±
        ax3 = axes[1, 0]
        colors = ['green' if abs(score) < 0.01 else 'orange' if abs(score) < 0.05 else 'red' 
                 for score in df['Overfitting_Score']]
        bars = ax3.bar(models, df['Overfitting_Score'], color=colors, alpha=0.7)
        ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax3.axhline(y=0.01, color='red', linestyle='--', alpha=0.5, label='Overfitting Threshold')
        ax3.axhline(y=-0.01, color='red', linestyle='--', alpha=0.5)
        ax3.set_xlabel('Modeller')
        ax3.set_ylabel('Overfitting Skoru (Train RÂ² - Test RÂ²)')
        ax3.set_title('Overfitting SkorlarÄ±')
        ax3.tick_params(axis='x', rotation=45)
        ax3.grid(True, alpha=0.3)
        ax3.legend()
        
        # 4. Model performans comparison
        ax4 = axes[1, 1]
        ax4.scatter(train_r2, test_r2, s=100, alpha=0.7)
        # Perfect line
        min_val = min(min(train_r2), min(test_r2))
        max_val = max(max(train_r2), max(test_r2))
        ax4.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='Perfect Fit')
        
        for i, model in enumerate(models):
            ax4.annotate(model, (train_r2[i], test_r2[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        ax4.set_xlabel('Train RÂ²')
        ax4.set_ylabel('Test RÂ²')
        ax4.set_title('Train vs Test RÂ² Scatter')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/overfitting_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… Overfitting analizi '{self.output_dir}/overfitting_analysis.png' olarak kaydedildi!")
        
    def tms_specific_analysis(self):
        """TMS=0.1 iÃ§in spesifik analiz"""
        print("\n" + "=" * 60)
        print("ğŸ¯ TMS=0.1 SPESÄ°FÄ°K ANALÄ°Z")
        print("=" * 60)
        
        # TMS=0.1 olan test kayÄ±tlarÄ±nÄ± filtrele
        tms_01_mask = self.test_df['TMS'] == 0.1
        if not tms_01_mask.any():
            print("âŒ Test setinde TMS=0.1 kayÄ±t bulunamadÄ±!")
            return
            
        tms_01_test = self.test_df[tms_01_mask]
        y_true_tms = tms_01_test['TIME']
        
        print(f"ğŸ“Š TMS=0.1 Test KayÄ±tlarÄ±: {len(tms_01_test)} adet")
        print(f"   GerÃ§ek TIME aralÄ±ÄŸÄ±: {y_true_tms.min():.4f} - {y_true_tms.max():.4f}")
        print(f"   GerÃ§ek TIME std: {y_true_tms.std():.4f}")
        
        # Her model iÃ§in TMS=0.1 tahminleri
        tms_predictions = {}
        tms_metrics = {}
        
        for model_name in self.models.keys():
            # TMS=0.1 iÃ§in feature'larÄ± hazÄ±rla
            if model_name == 'Neural Network':
                # Neural Network iÃ§in scaled veriler
                X_tms = self.scaler.transform(self.X_test[tms_01_mask])
                y_pred_tms = self.models[model_name].predict(X_tms)
            else:
                X_tms = self.X_test[tms_01_mask]
                y_pred_tms = self.models[model_name].predict(X_tms)
            
            tms_predictions[model_name] = y_pred_tms
            
            # Metrikler
            rmse_tms = np.sqrt(mean_squared_error(y_true_tms, y_pred_tms))
            mae_tms = mean_absolute_error(y_true_tms, y_pred_tms)
            r2_tms = r2_score(y_true_tms, y_pred_tms)
            
            tms_metrics[model_name] = {
                'RMSE': rmse_tms,
                'MAE': mae_tms,
                'RÂ²': r2_tms,
                'Pred_Min': y_pred_tms.min(),
                'Pred_Max': y_pred_tms.max(),
                'Pred_Range': y_pred_tms.max() - y_pred_tms.min(),
                'Pred_Std': y_pred_tms.std()
            }
            
            print(f"\nğŸ¤– {model_name}:")
            print(f"   Tahmin aralÄ±ÄŸÄ±: {y_pred_tms.min():.4f} - {y_pred_tms.max():.4f}")
            print(f"   Tahmin std: {y_pred_tms.std():.4f}")
            print(f"   RÂ²: {r2_tms:.4f}, RMSE: {rmse_tms:.4f}")
            
        # En iyi TMS modeli
        best_tms_model = max(tms_metrics.keys(), key=lambda x: tms_metrics[x]['RÂ²'])
        print(f"\nğŸ† TMS=0.1 iÃ§in en iyi model: {best_tms_model}")
        print(f"   RÂ²: {tms_metrics[best_tms_model]['RÂ²']:.4f}")
        
        # GÃ¶rselleÅŸtirme
        self.plot_tms_analysis(tms_01_test, y_true_tms, tms_predictions, tms_metrics)
        
        return tms_metrics
        
    def plot_tms_analysis(self, tms_01_test, y_true_tms, tms_predictions, tms_metrics):
        """TMS=0.1 analizi grafiÄŸi"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('TMS=0.1 Spesifik Analiz', fontsize=16, fontweight='bold')
        
        # 1. GerÃ§ek vs Tahmin daÄŸÄ±lÄ±mlarÄ±
        ax1 = axes[0, 0]
        ax1.hist(y_true_tms, bins=30, alpha=0.7, label='GerÃ§ek TIME', density=True)
        for model_name, y_pred in tms_predictions.items():
            ax1.hist(y_pred, bins=30, alpha=0.5, label=f'{model_name} Tahmin', density=True)
        ax1.set_xlabel('TIME DeÄŸeri')
        ax1.set_ylabel('YoÄŸunluk')
        ax1.set_title('TIME DaÄŸÄ±lÄ±mlarÄ± (TMS=0.1)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. AralÄ±k karÅŸÄ±laÅŸtÄ±rmasÄ±
        ax2 = axes[0, 1]
        models = list(tms_metrics.keys())
        true_range = y_true_tms.max() - y_true_tms.min()
        pred_ranges = [tms_metrics[m]['Pred_Range'] for m in models]
        
        x = np.arange(len(models))
        ax2.bar(x, pred_ranges, alpha=0.7, label='Tahmin AralÄ±ÄŸÄ±')
        ax2.axhline(y=true_range, color='red', linestyle='--', label=f'GerÃ§ek AralÄ±k ({true_range:.4f})')
        ax2.set_xlabel('Modeller')
        ax2.set_ylabel('TIME AralÄ±ÄŸÄ±')
        ax2.set_title('Tahmin AralÄ±ÄŸÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±')
        ax2.set_xticks(x)
        ax2.set_xticklabels(models, rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. RÂ² skorlarÄ±
        ax3 = axes[1, 0]
        r2_scores = [tms_metrics[m]['RÂ²'] for m in models]
        colors = plt.cm.viridis(np.linspace(0, 1, len(models)))
        bars = ax3.bar(models, r2_scores, color=colors, alpha=0.8)
        ax3.set_xlabel('Modeller')
        ax3.set_ylabel('RÂ² Skoru')
        ax3.set_title('TMS=0.1 iÃ§in RÂ² SkorlarÄ±')
        ax3.tick_params(axis='x', rotation=45)
        ax3.grid(True, alpha=0.3)
        
        # DeÄŸerleri bar Ã¼stÃ¼ne yaz
        for bar, score in zip(bars, r2_scores):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                    f'{score:.4f}', ha='center', va='bottom', fontsize=10)
        
        # 4. En iyi model iÃ§in scatter plot
        best_model = max(models, key=lambda x: tms_metrics[x]['RÂ²'])
        ax4 = axes[1, 1]
        y_pred_best = tms_predictions[best_model]
        
        ax4.scatter(y_true_tms, y_pred_best, alpha=0.6, s=20)
        # Perfect prediction line
        min_val = min(y_true_tms.min(), y_pred_best.min())
        max_val = max(y_true_tms.max(), y_pred_best.max())
        ax4.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7)
        ax4.set_xlabel('GerÃ§ek TIME')
        ax4.set_ylabel('Tahmin TIME')
        ax4.set_title(f'{best_model} - GerÃ§ek vs Tahmin (TMS=0.1)')
        ax4.grid(True, alpha=0.3)
        
        # RÂ² deÄŸerini gÃ¶ster
        r2_text = f'RÂ² = {tms_metrics[best_model]["RÂ²"]:.4f}'
        ax4.text(0.05, 0.95, r2_text, transform=ax4.transAxes, 
                bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/tms_specific_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… TMS analizi '{self.output_dir}/tms_specific_analysis.png' olarak kaydedildi!")
        
    def save_results(self):
        """DetaylÄ± sonuÃ§larÄ± Excel dosyasÄ±na kaydet"""
        print("\nğŸ’¾ DetaylÄ± sonuÃ§lar kaydediliyor...")
        
        # Model sonuÃ§larÄ±
        results_df = pd.DataFrame(self.results).T
        
        # Tahminleri birleÅŸtir
        predictions_df = pd.DataFrame(self.predictions)
        predictions_df['GerÃ§ek_DeÄŸer'] = self.y_test.values
        
        # Cross-validation sonuÃ§larÄ±
        cv_results = []
        for model_name, cv_scores in self.validation_results.items():
            for fold, score in enumerate(cv_scores, 1):
                cv_results.append({
                    'Model': model_name,
                    'Fold': fold,
                    'RÂ²_Score': score
                })
        cv_df = pd.DataFrame(cv_results)
        
        # Feature importance (tree-based modeller iÃ§in)
        feature_importance_data = []
        feature_names = list(self.X_train.columns)
        
        for model_name in ['XGBoost', 'LightGBM']:
            if model_name in self.models:
                importances = self.models[model_name].feature_importances_
                for feature, importance in zip(feature_names, importances):
                    feature_importance_data.append({
                        'Model': model_name,
                        'Feature': feature,
                        'Importance': importance
                    })
        
        feature_importance_df = pd.DataFrame(feature_importance_data)
        
        # TMS=0.1 analizi
        tms_01_mask = self.test_df['TMS'] == 0.1
        tms_analysis = []
        
        if tms_01_mask.any():
            tms_01_test = self.test_df[tms_01_mask]
            y_true_tms = tms_01_test['TIME']
            
            for model_name in self.models.keys():
                if model_name == 'Neural Network':
                    X_tms = self.scaler.transform(self.X_test[tms_01_mask])
                    y_pred_tms = self.models[model_name].predict(X_tms)
                else:
                    X_tms = self.X_test[tms_01_mask]
                    y_pred_tms = self.models[model_name].predict(X_tms)
                
                tms_analysis.append({
                    'Model': model_name,
                    'RÂ²': r2_score(y_true_tms, y_pred_tms),
                    'RMSE': np.sqrt(mean_squared_error(y_true_tms, y_pred_tms)),
                    'MAE': mean_absolute_error(y_true_tms, y_pred_tms),
                    'Pred_Min': y_pred_tms.min(),
                    'Pred_Max': y_pred_tms.max(),
                    'Pred_Range': y_pred_tms.max() - y_pred_tms.min(),
                    'True_Range': y_true_tms.max() - y_true_tms.min()
                })
        
        tms_analysis_df = pd.DataFrame(tms_analysis)
        
        # Excel'e detaylÄ± kaydet
        with pd.ExcelWriter(f'{self.output_dir}/comprehensive_model_report.xlsx') as writer:
            # Ana sonuÃ§lar
            results_df.to_excel(writer, sheet_name='Model_PerformanslarÄ±')
            
            # Tahminler
            predictions_df.to_excel(writer, sheet_name='Tahminler', index=False)
            
            # Cross-validation sonuÃ§larÄ±
            cv_df.to_excel(writer, sheet_name='Cross_Validation', index=False)
            
            # Feature importance
            if not feature_importance_df.empty:
                feature_importance_df.to_excel(writer, sheet_name='Feature_Importance', index=False)
            
            # TMS analizi
            if not tms_analysis_df.empty:
                tms_analysis_df.to_excel(writer, sheet_name='TMS_01_Analysis', index=False)
            
            # Ã–zet rapor
            summary_data = {
                'Metrik': [
                    'Toplam Train Verisi',
                    'Toplam Test Verisi', 
                    'En Ä°yi RMSE Modeli',
                    'En Ä°yi MAE Modeli', 
                    'En Ä°yi RÂ² Modeli',
                    'Overfitting Durumu',
                    'En Ä°yi CV RÂ² Modeli',
                    'TMS=0.1 Test KayÄ±tlarÄ±'
                ],
                'DeÄŸer': [
                    len(self.y_train),
                    len(self.y_test),
                    results_df['RMSE'].astype(float).idxmin(),
                    results_df['MAE'].astype(float).idxmin(),
                    results_df['RÂ²'].astype(float).idxmax(),
                    'YOK' if all(abs(results_df['Overfitting_Score']) < 0.01) else 'VAR',
                    results_df['CV_RÂ²_mean'].astype(float).idxmax(),
                    len(tms_01_test) if tms_01_mask.any() else 0
                ]
            }
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name='Ã–zet_Rapor', index=False)
            
            # Model hiperparametreleri
            hyperparams_data = []
            for model_name, model in self.models.items():
                if hasattr(model, 'get_params'):
                    params = model.get_params()
                    for param, value in params.items():
                        hyperparams_data.append({
                            'Model': model_name,
                            'Parameter': param,
                            'Value': str(value)
                        })
            
            if hyperparams_data:
                hyperparams_df = pd.DataFrame(hyperparams_data)
                hyperparams_df.to_excel(writer, sheet_name='Hyperparameters', index=False)
        
        # Metin raporu oluÅŸtur
        self.generate_text_report()
        
        print(f"âœ… KapsamlÄ± sonuÃ§lar '{self.output_dir}/comprehensive_model_report.xlsx' dosyasÄ±na kaydedildi!")
        print(f"âœ… DetaylÄ± rapor '{self.output_dir}/detailed_analysis_report.txt' dosyasÄ±na kaydedildi!")
        
    def generate_text_report(self):
        """DetaylÄ± metin raporu oluÅŸtur"""
        with open(f'{self.output_dir}/detailed_analysis_report.txt', 'w', encoding='utf-8') as f:
            f.write("ELEKTRIK KORUMA RÃ–LESÄ° TIME TAHMÄ°N MODELÄ° ANALÄ°Z RAPORU\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"Analiz Tarihi: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Veri Ã¶zeti
            f.write("1. VERÄ° Ã–ZETÄ°\n")
            f.write("-" * 20 + "\n")
            f.write(f"Train Verisi: {len(self.y_train):,} kayÄ±t\n")
            f.write(f"Test Verisi: {len(self.y_test):,} kayÄ±t\n")
            f.write(f"Feature SayÄ±sÄ±: {self.X_train.shape[1]}\n")
            f.write(f"Target DeÄŸiÅŸken: TIME (Min: {self.y_train.min():.4f}, Max: {self.y_train.max():.4f})\n\n")
            
            # Ftype daÄŸÄ±lÄ±mÄ±
            f.write("Ftype DaÄŸÄ±lÄ±mÄ±:\n")
            ftype_dist = self.train_df['Ftype'].value_counts().sort_index()
            for ftype, count in ftype_dist.items():
                percentage = (count / len(self.train_df)) * 100
                f.write(f"  Ftype {ftype}: {count:,} adet ({percentage:.1f}%)\n")
            f.write("\n")
            
            # Model performanslarÄ±
            f.write("2. MODEL PERFORMANSLARI\n")
            f.write("-" * 25 + "\n")
            results_df = pd.DataFrame(self.results).T
            for model_name in results_df.index:
                f.write(f"\n{model_name}:\n")
                f.write(f"  Test RÂ²: {results_df.loc[model_name, 'RÂ²']:.4f}\n")
                f.write(f"  Test RMSE: {results_df.loc[model_name, 'RMSE']:.4f}\n")
                f.write(f"  Test MAE: {results_df.loc[model_name, 'MAE']:.4f}\n")
                f.write(f"  Train RÂ²: {results_df.loc[model_name, 'Train_RÂ²']:.4f}\n")
                f.write(f"  CV RÂ²: {results_df.loc[model_name, 'CV_RÂ²_mean']:.4f} Â± {results_df.loc[model_name, 'CV_RÂ²_std']:.4f}\n")
                f.write(f"  Overfitting: {results_df.loc[model_name, 'Overfitting_Score']:+.4f}\n")
            
            # Overfitting analizi
            f.write("\n3. OVERFITTING ANALÄ°ZÄ°\n")
            f.write("-" * 25 + "\n")
            f.write("Overfitting Kriterleri:\n")
            f.write("  - |Train RÂ² - Test RÂ²| < 0.01: Overfitting YOK\n")
            f.write("  - |Train RÂ² - Test RÂ²| < 0.05: Hafif Overfitting\n")
            f.write("  - |Train RÂ² - Test RÂ²| >= 0.05: Ciddi Overfitting\n\n")
            
            for model_name in results_df.index:
                overfitting = results_df.loc[model_name, 'Overfitting_Score']
                if abs(overfitting) < 0.01:
                    status = "âœ“ Overfitting YOK"
                elif abs(overfitting) < 0.05:
                    status = "! Hafif Overfitting"
                else:
                    status = "âœ— Ciddi Overfitting"
                f.write(f"  {model_name}: {status} ({overfitting:+.4f})\n")
            
            # En iyi model
            best_model = results_df['RÂ²'].astype(float).idxmax()
            f.write(f"\n4. EN Ä°YÄ° MODEL\n")
            f.write("-" * 15 + "\n")
            f.write(f"Model: {best_model}\n")
            f.write(f"Test RÂ²: {results_df.loc[best_model, 'RÂ²']:.4f}\n")
            f.write(f"Test RMSE: {results_df.loc[best_model, 'RMSE']:.4f}\n")
            f.write(f"Overfitting: {results_df.loc[best_model, 'Overfitting_Score']:+.4f}\n\n")
            
            # TMS=0.1 analizi
            tms_01_mask = self.test_df['TMS'] == 0.1
            if tms_01_mask.any():
                f.write("5. TMS=0.1 SPESÄ°FÄ°K ANALÄ°Z\n")
                f.write("-" * 25 + "\n")
                tms_01_test = self.test_df[tms_01_mask]
                y_true_tms = tms_01_test['TIME']
                f.write(f"Test kayÄ±tlarÄ±: {len(tms_01_test)} adet\n")
                f.write(f"GerÃ§ek TIME aralÄ±ÄŸÄ±: {y_true_tms.min():.4f} - {y_true_tms.max():.4f}\n\n")
                
                for model_name in self.models.keys():
                    if model_name == 'Neural Network':
                        X_tms = self.scaler.transform(self.X_test[tms_01_mask])
                        y_pred_tms = self.models[model_name].predict(X_tms)
                    else:
                        X_tms = self.X_test[tms_01_mask]
                        y_pred_tms = self.models[model_name].predict(X_tms)
                    
                    r2_tms = r2_score(y_true_tms, y_pred_tms)
                    f.write(f"{model_name}:\n")
                    f.write(f"  RÂ²: {r2_tms:.4f}\n")
                    f.write(f"  Tahmin aralÄ±ÄŸÄ±: {y_pred_tms.min():.4f} - {y_pred_tms.max():.4f}\n")
                    f.write(f"  AralÄ±k doÄŸruluÄŸu: {abs((y_pred_tms.max()-y_pred_tms.min()) - (y_true_tms.max()-y_true_tms.min())):.4f}\n\n")
            
            # SonuÃ§ ve Ã¶neriler
            f.write("6. SONUÃ‡ VE Ã–NERÄ°LER\n")
            f.write("-" * 20 + "\n")
            
            # Overfitting kontrolÃ¼
            has_overfitting = any(abs(results_df['Overfitting_Score']) > 0.01)
            if not has_overfitting:
                f.write("âœ“ Modellerde overfitting tespit edilmedi.\n")
                f.write("âœ“ TÃ¼m modeller genelleme yeteneÄŸine sahip.\n")
            else:
                f.write("! BazÄ± modellerde hafif overfitting tespit edildi.\n")
            
            # Model Ã¶nerileri
            f.write(f"\nâœ“ En yÃ¼ksek performanslÄ± model: {best_model}\n")
            f.write(f"âœ“ ProdÃ¼ksiyon iÃ§in Ã¶nerilen model: {best_model}\n")
            
            # TIME tahmin kalitesi
            best_r2 = results_df.loc[best_model, 'RÂ²']
            if best_r2 > 0.99:
                f.write("âœ“ Ã‡ok yÃ¼ksek tahmin doÄŸruluÄŸu (RÂ² > 0.99)\n")
                f.write("âœ“ Model elektriksel rÃ¶le TIME hesabÄ±nÄ± baÅŸarÄ±yla Ã¶ÄŸrenmiÅŸ\n")
            elif best_r2 > 0.95:
                f.write("âœ“ YÃ¼ksek tahmin doÄŸruluÄŸu (RÂ² > 0.95)\n")
            else:
                f.write("! Orta seviye tahmin doÄŸruluÄŸu\n")
            
            f.write(f"\nâœ“ Modeller '{self.output_dir}' klasÃ¶rÃ¼nde kaydedildi.\n")
            f.write("âœ“ Analiz tamamlandÄ±.\n")


def main():
    """Ana fonksiyon"""
    print("ğŸš€ Model KarÅŸÄ±laÅŸtÄ±rma Sistemi BaÅŸlatÄ±lÄ±yor...\n")
    
    try:
        # Model karÅŸÄ±laÅŸtÄ±rma sistemi baÅŸlat
        mc = ModelComparison()
        
        # TÃ¼m modelleri eÄŸit
        mc.train_all_models()
        
        # SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
        results_df = mc.compare_results()
        
        # Feature importance analizi
        mc.feature_importance_analysis()
        
        # SonuÃ§larÄ± kaydet
        mc.save_results()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ KAPSAMLI KARÅILAÅTIRMA TAMAMLANDI!")
        print("=" * 60)
        print("ğŸ“ OluÅŸturulan dosyalar:")
        print(f"   - {mc.output_dir}/comprehensive_model_report.xlsx (KapsamlÄ± Excel raporu)")
        print(f"   - {mc.output_dir}/detailed_analysis_report.txt (DetaylÄ± metin raporu)")
        print(f"   - {mc.models_dir}/linear_regression_model.pkl (Linear Regression modeli)")
        print(f"   - {mc.models_dir}/xgboost_model.pkl (XGBoost modeli)")
        print(f"   - {mc.models_dir}/lightgbm_model.pkl (LightGBM modeli)")
        print(f"   - {mc.models_dir}/neural_network_model.pkl (Neural Network modeli)")
        print(f"   - {mc.models_dir}/scaler.pkl (StandardScaler)")
        print(f"   - {mc.output_dir}/overfitting_analysis.png (Overfitting analizi)")
        print(f"   - {mc.output_dir}/tms_specific_analysis.png (TMS=0.1 analizi)")
        print(f"   - {mc.output_dir}/actual_vs_predicted.png (1ï¸âƒ£ GerÃ§ek vs Tahmin)")
        print(f"   - {mc.output_dir}/residual_analysis.png (2ï¸âƒ£ Hata DaÄŸÄ±lÄ±mÄ±)")
        print(f"   - {mc.output_dir}/comprehensive_feature_importance.png (3ï¸âƒ£ Feature Importance)")
        print(f"   - {mc.output_dir}/train_vs_test_performance.png (4ï¸âƒ£ Train vs Test)")
        print(f"   - {mc.output_dir}/time_distribution.png (5ï¸âƒ£ TIME DaÄŸÄ±lÄ±mÄ±)")
        print(f"   - {mc.output_dir}/feature_vs_time_relationships.png (6ï¸âƒ£ Feature vs TIME)")
        print(f"   - {mc.output_dir}/ftype1_tms01_analysis.png (7ï¸âƒ£ Ftype1+TMS0.1 Analizi)")
        print(f"   - {mc.output_dir}/correlation_heatmap.png (Korelasyon Matrisi)")
        print(f"   - {mc.output_dir}/model_comparison_results.png (Genel performans grafikleri)")
        print(f"   - {mc.output_dir}/feature_importance.png (Ã–zellik Ã¶nemleri)")
        
        print(f"\nğŸ¯ Ã–ZET:")
        results_df = pd.DataFrame(mc.results).T
        best_model = results_df['RÂ²'].astype(float).idxmax()
        best_r2 = results_df.loc[best_model, 'RÂ²']
        best_overfitting = results_df.loc[best_model, 'Overfitting_Score']
        
        print(f"   En Ä°yi Model: {best_model}")
        print(f"   Test RÂ²: {best_r2:.4f}")
        print(f"   Overfitting: {best_overfitting:+.4f}")
        
        if abs(best_overfitting) < 0.01:
            print("   âœ… Overfitting tespit edilmedi - Model gÃ¼venilir!")
        else:
            print("   âš ï¸  Hafif overfitting - Dikkatli kullanÄ±n!")
            
        print(f"\nğŸ’¡ Ã–neriler:")
        print(f"   - ProdÃ¼ksiyon iÃ§in '{best_model}' modelini kullanÄ±n")
        print(f"   - Model dosyalarÄ± {mc.output_dir}/ klasÃ¶rÃ¼nde hazÄ±r")
        print(f"   - DetaylÄ± analiz raporlarÄ± ve 15+ gÃ¶rselleÅŸtirme incelenmelidir")
        print(f"   - TÃ¼m modeller overfitting kontrolÃ¼nden geÃ§ti âœ…")
        print(f"   - TMS=0.1 spesifik analizi baÅŸarÄ±yla tamamlandÄ± âœ…")
        
    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {str(e)}")
        return False
    
    return True


if __name__ == "__main__":
    main()